{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5130ae-9faf-48c6-ab45-9610a6781463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель размещена на: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#### не помещается\n",
    "model_name = \"Vikhrmodels/Vikhr-Llama-3.2-1B-instruct\" \n",
    "\n",
    "# Загрузка токенайзера\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Загрузка модели и помещение на GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "\n",
    "def empty_gpu_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "\n",
    "# Проверка устройства\n",
    "print(f\"Модель размещена на: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3d8975-8c2a-4660-a627-26bdb27b4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Архитектура модели\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffe96b6-b463-4c4d-90c8-6ff9bf2dedaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: model.embed_tokens.weight\n",
      "Shape: torch.Size([128256, 2048])\n",
      "Values: tensor([[ 0.0031,  0.0178,  0.0210,  ..., -0.0052, -0.0420, -0.0334],\n",
      "        [ 0.0237, -0.0229,  0.0199,  ..., -0.0095, -0.0022, -0.0396],\n",
      "        [ 0.0145,  0.0106,  0.0098,  ...,  0.0068, -0.0116,  0.0058],\n",
      "        [-0.0219,  0.0072, -0.0017,  ...,  0.0031,  0.0103, -0.0023],\n",
      "        [ 0.0234, -0.0337, -0.0315,  ..., -0.0240,  0.0016, -0.0067]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Parameter name: model.layers.0.self_attn.q_proj.weight\n",
      "Shape: torch.Size([2048, 2048])\n",
      "Values: tensor([[-0.0183,  0.0056,  0.0255,  ..., -0.0079, -0.0135,  0.0201],\n",
      "        [ 0.0115,  0.0589,  0.0556,  ..., -0.0330, -0.0160,  0.0111],\n",
      "        [ 0.0177,  0.0158,  0.0342,  ..., -0.0387, -0.0378, -0.0280],\n",
      "        [-0.0069, -0.0441, -0.0418,  ...,  0.0210,  0.0214,  0.0312],\n",
      "        [-0.0294, -0.1022, -0.0605,  ...,  0.0638,  0.0403,  0.0161]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Parameter name: model.layers.0.self_attn.k_proj.weight\n",
      "Shape: torch.Size([512, 2048])\n",
      "Values: tensor([[ 0.0559,  0.1185,  0.0621,  ..., -0.1018, -0.0334,  0.0208],\n",
      "        [-0.0123,  0.0746,  0.0576,  ..., -0.0253, -0.0217, -0.0443],\n",
      "        [-0.0753, -0.0383, -0.0016,  ...,  0.0010, -0.0186, -0.0001],\n",
      "        [-0.0254, -0.0566, -0.0222,  ...,  0.0703,  0.0390,  0.0033],\n",
      "        [-0.0072, -0.0408, -0.0193,  ...,  0.0353,  0.0302,  0.0226]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Parameter name: model.layers.0.self_attn.v_proj.weight\n",
      "Shape: torch.Size([512, 2048])\n",
      "Values: tensor([[ 0.0146,  0.0006,  0.0113,  ...,  0.0088, -0.0051, -0.0086],\n",
      "        [ 0.0050, -0.0031, -0.0091,  ...,  0.0013,  0.0082, -0.0051],\n",
      "        [-0.0010,  0.0051, -0.0046,  ...,  0.0019,  0.0035,  0.0055],\n",
      "        [ 0.0025, -0.0104, -0.0074,  ...,  0.0015, -0.0042, -0.0079],\n",
      "        [-0.0139, -0.0083,  0.0036,  ..., -0.0169, -0.0057,  0.0032]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Parameter name: model.layers.0.self_attn.o_proj.weight\n",
      "Shape: torch.Size([2048, 2048])\n",
      "Values: tensor([[ 0.0002,  0.0060,  0.0133,  ..., -0.0090, -0.0018,  0.0108],\n",
      "        [ 0.0084,  0.0077, -0.0068,  ..., -0.0009,  0.0024, -0.0014],\n",
      "        [-0.0055, -0.0501, -0.0178,  ..., -0.0091, -0.0035, -0.0003],\n",
      "        [-0.0003, -0.0072, -0.0097,  ..., -0.0087,  0.0094,  0.0023],\n",
      "        [-0.0024,  0.0006, -0.0071,  ...,  0.0023,  0.0044, -0.0035]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if i<5:\n",
    "        print(f\"Parameter name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Values: {param[:5]}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9efb32-befb-4723-8b99-45e51e28c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1235814400\n",
      "Trainable parameters: 1235814400\n"
     ]
    }
   ],
   "source": [
    "#Количество параметров\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716ee259-0929-4843-b410-60b00310c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(prompt, tokens_number = 1512, temp = 0.3, top_k = 50, top_p = 0.95):\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], truncation=True,\n",
    "                                          add_generation_prompt=False, padding=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=tokens_number,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "\n",
    "    answer = decoded_output.split('<|start_header_id|>assistant<|end_header_id|>')[-1].replace('<|eot_id|>', '').strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150a3ae1-a915-4bee-a189-be39108d5d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Промпт:\n",
      "Сколько будет 2+2*2?\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Ответ:\n",
      "\n",
      "Для решения данной математической задачи выполним следующие шаги:\n",
      "\n",
      "1. Сначала умножим 2 на 2, чтобы получить результат умножения.\n",
      "2. Затем сложим полученный результат с 2.\n",
      "\n",
      "Итак, шаг за шагом:\n",
      "\n",
      "1. \\(2 \\times 2 = 4\\)\n",
      "2. \\(4 + 2 = 6\\)\n",
      "\n",
      "Таким образом, \\(2 + 2 \\times 2 = 6\\).\n"
     ]
    }
   ],
   "source": [
    "print('Промпт:')\n",
    "prompt = 'Сколько будет 2+2*2?'\n",
    "print(prompt)\n",
    "print()\n",
    "print()\n",
    "print(100*'-')\n",
    "print('Ответ:')\n",
    "print()\n",
    "print(get_answer(prompt, temp = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f86a3-a8b5-4ba1-be6f-4945c94564e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d3da7-0f8b-4d48-8c78-8eabedcee531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb162f-fc67-4178-8f1f-0a82d5c6fd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bf3ea-0884-43be-aea7-2bf81f9e835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1dc3bb-6173-46aa-866b-4138964790a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae87a9-bf76-4081-b798-f60d3258351b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96972b10-a425-416f-b9a8-0ecc578c774b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a612df5-90ad-4645-b61d-f28b304c51fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "LLM(prompt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf288b-90e9-44b6-a5c5-1d53b9ff3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text -> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625035c8-794c-40b4-8cfd-9134e2638568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cc476ad-b9ad-4d33-b25f-8b0cc7179701",
   "metadata": {},
   "source": [
    "### Не знает что такое ГЗ в бауманке. Нужен будет датасет состоящий из сленга и дообучиться на нём.\n",
    "### Не понимает что дом физики находится внутри гз и отвечает невпопад. \n",
    "## Но в целом модель понимает суть вопросов, структура ответа правильная, нужно только дополнить знаниями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7f3eb-3e9c-43fd-a28b-5e85c10b7c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
